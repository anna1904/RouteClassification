{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_completed_at        0\n",
      "stop_arrived_at          0\n",
      "stop_latest              0\n",
      "stop_earliest            0\n",
      "stop_dispatched_at       0\n",
      "location_id              0\n",
      "location_type_id         0\n",
      "driver_workday_id        0\n",
      "organization_id          0\n",
      "address_id               0\n",
      "location_is_depot        0\n",
      "driver_id                0\n",
      "contact_id               0\n",
      "current_lat              0\n",
      "current_lng              0\n",
      "prev_planned_lat      7450\n",
      "prev_planned_lng      7450\n",
      "prev_actual_lat       7450\n",
      "prev_actual_lng       7450\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_stops = pd.read_csv('uni_molde_v2.csv', sep=',')\n",
    "\n",
    "data_stops\n",
    "data_stops.loc[data_stops['stop_completed_at'].isna(), 'stop_completed_at'] = \"-1\"\n",
    "data_stops.loc[data_stops['stop_arrived_at'].isna(), 'stop_arrived_at'] = \"-1\"\n",
    "\n",
    "print(data_stops.isnull().sum())\n",
    "\n",
    "\n",
    "sorted_data_stops = data_stops.sort_values(by='stop_dispatched_at', ascending=True)\n",
    "sorted_data_stops = sorted_data_stops.reset_index(drop=True)\n",
    "sorted_data_stops['day_of_week'] = pd.to_datetime(sorted_data_stops['stop_dispatched_at']).dt.day_name()\n",
    "sorted_data_stops['date'] = pd.to_datetime(sorted_data_stops['stop_dispatched_at']).dt.date\n",
    "\n",
    "#clustering\n",
    "locations_df = sorted_data_stops[['current_lat', 'current_lng']]\n",
    "kmeans = KMeans(n_clusters=1000, random_state=42)\n",
    "kmeans.fit(locations_df)\n",
    "sorted_data_stops['cluster'] = kmeans.labels_ + 1\n",
    "\n",
    "sorted_data_stops['location_id_craft'] = sorted_data_stops.groupby(['current_lat', 'current_lng']).ngroup()+1\n",
    "with open('output.txt', 'w') as f:\n",
    "    print(sorted_data_stops.to_string(), file=f)\n",
    "print('number of groups', sorted_data_stops['location_id_craft'].nunique())\n",
    "\n",
    "# data_stops_day= sorted_data_stops[sorted_data_stops['day_of_week'] == \"Wednesday\"]\n",
    "grouped_df = sorted_data_stops.groupby('driver_workday_id')[['driver_id', 'location_type_id', 'address_id', 'stop_dispatched_at', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng', 'stop_completed_at', 'cluster', 'location_id_craft','day_of_week', 'date']].apply(lambda x: pd.Series({\n",
    "    'driver_id': x['driver_id'].tolist(),\n",
    "    'location_type_id': x['location_type_id'].tolist(),\n",
    "    'planned_route_location': x['address_id'].tolist(),\n",
    "    'stop_dispatched_at': x['stop_dispatched_at'].tolist(),\n",
    "    'stop_arrived_at': x['stop_arrived_at'].tolist(),\n",
    "    'stop_earliest': x['stop_earliest'].tolist(),\n",
    "    'stop_latest': x['stop_latest'].tolist(),\n",
    "    'current_lat': x['current_lat'].tolist(),\n",
    "    'current_lng': x['current_lng'].tolist(),\n",
    "    'stop_completed_at': x['stop_completed_at'].tolist(),\n",
    "    'planned_route_cluster': x['cluster'].tolist(),\n",
    "    'planned_route_craft': x['location_id_craft'].tolist(),\n",
    "    'day_of_week': x['day_of_week'].tolist(),\n",
    "    'date': x['date'].tolist()\n",
    "})).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate distance between two coordinates\n",
    "def calculate_distance(row):\n",
    "    distances = []\n",
    "    for i in range(len(row['planned_route_craft'])-1):\n",
    "        coords_1 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i], ['current_lng']].values[0][0])\n",
    "        coords_2 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i+1], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i+1], ['current_lng']].values[0][0])\n",
    "        distances.append(geodesic(coords_1, coords_2).miles)\n",
    "    return distances\n",
    "\n",
    "# Create a new column 'distance_route' in 'final_routes'\n",
    "grouped_df['distance_route'] = grouped_df.apply(calculate_distance, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes = grouped_df[grouped_df.apply(lambda row: max(row['stop_dispatched_at']) < min(row['stop_completed_at']), axis=1)]\n",
    "routes = routes.reset_index(drop=True)\n",
    "len(routes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index_routes_with_na = []\n",
    "for i in range(len(routes)):\n",
    "    row = routes.iloc[i]\n",
    "    if \"-1\" in row['stop_arrived_at']:\n",
    "        index_routes_with_na.append(i)\n",
    "print(\"The number of routes where one value is NA(arrived time)\", len(index_routes_with_na))\n",
    "print(index_routes_with_na)\n",
    "routes = routes.drop(index_routes_with_na)\n",
    "routes.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check if it is the same driver in the route\n",
    "for i in routes['driver_id']:\n",
    "    if not all(x == i[0] for x in i):\n",
    "            print('Not the same driver in the route')\n",
    "routes['driver_id'] = routes['driver_id'].apply(lambda x : x[0])\n",
    "\n",
    "routes['day_of_week'] = routes['day_of_week'].apply(lambda x : x[0])\n",
    "routes['date'] = routes['date'].apply(lambda x : x[0])\n",
    "\n",
    "routes['date'] = pd.to_datetime(routes['date'])\n",
    "routes['last_two_weeks_count'] = routes.apply(lambda row:\n",
    "                                      routes[(routes['driver_id'] == row['driver_id']) &\n",
    "                                         (row['date'] - routes['date']).dt.days.between(-14, 0)].shape[0],\n",
    "                                      axis=1)\n",
    "routes = routes[(routes['date'] < '2024-01-01') | (routes['date'] > '2024-01-14')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_actual_route(df, column):\n",
    "    res_col = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        sorted_dates = sorted(row['stop_arrived_at'])\n",
    "        mapping = {}\n",
    "        for i, date in enumerate(sorted_dates):\n",
    "            mapping[date] = row[column][i]\n",
    "        res_val = [mapping[row['stop_arrived_at'][i]] for i in range(len(row['stop_arrived_at']))]\n",
    "        res_col.append(res_val)\n",
    "    return res_col\n",
    "\n",
    "routes['actual_route_location'] = create_actual_route(routes, 'planned_route_craft')\n",
    "routes.to_csv('routes.csv', sep=';')\n",
    "routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "planned_routes = routes[['planned_route_craft', 'actual_route_location', 'driver_id', 'day_of_week', 'distance_route', 'last_two_weeks_count', 'location_type_id', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng']]\n",
    "actual_routes = routes['actual_route_location']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "planned_routes_list = planned_routes['planned_route_craft'].tolist()\n",
    "actual_routes_list = actual_routes.tolist()\n",
    "\n",
    "# filtered_rows = []\n",
    "#\n",
    "# for row in actual_routes_list:\n",
    "#     if row not in planned_routes_list:\n",
    "#         filtered_rows.append(row)\n",
    "\n",
    "#duplicated removed\n",
    "# planned_routes_list = [array for i, array in enumerate(planned_routes_list) if array not in planned_routes_list[:i]]\n",
    "# actual_routes_list = [array for i, array in enumerate(filtered_rows) if array not in filtered_rows[:i]]\n",
    "len(planned_routes_list),len(actual_routes_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Edit distance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Edit Distance\n",
    "\n",
    "def minDistance(word1, word2) -> int:\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    # dp[i][j] := min # Of operations to convert word1[0..i) to word2[0..j)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "      dp[i][0] = i\n",
    "\n",
    "    for j in range(1, n + 1):\n",
    "      dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "      for j in range(1, n + 1):\n",
    "        if word1[i - 1] == word2[j - 1]:\n",
    "          dp[i][j] = dp[i - 1][j - 1]\n",
    "        else:\n",
    "          dp[i][j] = min(dp[i - 1][j - 1], dp[i - 1][j], dp[i][j - 1]) + 1\n",
    "\n",
    "    return dp[m][n]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each driver-day, we consider only the planned route. For each planned route we calculate how much it deviates from the actual route (e.g., use some form of edit distance - see https://link.springer.com/article/10.1007/s10732-006-9001-3?), which is then normalized (divide by max edit distance, or number of visits or something?) to the interval [0, 1]. Then, a planned route is good if this distance (between planned and actual) is less than a certain threshold and otherwise it is bad. This becomes the label (bad/good)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "completed_routes_df = pd.DataFrame(columns=['planned_route_craft', 'actual_route_location', 'driver_id', 'day_of_week', 'distance_route', 'last_two_weeks_count', 'location_type_id', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng', 'edit_distance'])\n",
    "uncompleted_routes_df = pd.DataFrame(columns=['planned_route_craft', 'actual_route_location', 'driver_id', 'day_of_week', 'distance_route', 'last_two_weeks_count', 'location_type_id', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'current_lat', 'current_lng', 'edit_distance'])\n",
    "#11700795\n",
    "for i in range(len(planned_routes_list)):\n",
    "    editDistance = minDistance(planned_routes_list[i],actual_routes_list[i])\n",
    "    if editDistance < 6:\n",
    "        completed_routes_df = pd.concat([completed_routes_df, pd.DataFrame([planned_routes.iloc[i]]).assign(edit_distance=editDistance).reset_index(drop=True)], ignore_index=True)\n",
    "    else:\n",
    "        uncompleted_routes_df = pd.concat([uncompleted_routes_df, pd.DataFrame([planned_routes.iloc[i]]).assign(edit_distance=editDistance).reset_index(drop=True)], ignore_index=True)\n",
    "\n",
    "uncompleted_routes_df.to_csv('uncompleted_routes_df.txt', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import folium\n",
    "def build_route(lat_seq, lon_seq, color, earliest, latest, arrived, location_type_id, file_name):\n",
    "    m = folium.Map(location=[lat_seq[0], lon_seq[0]], zoom_start=13)\n",
    "\n",
    "    # Add a polyline with arrows\n",
    "    offset = 0\n",
    "    for i in range(len(lat_seq)):\n",
    "        lat, lon = lat_seq[i], lon_seq[i]\n",
    "        earliest_date = earliest[i]\n",
    "        latest_date = latest[i]\n",
    "        arrived_date = arrived[i]\n",
    "        location_type = location_type_id[i]\n",
    "\n",
    "        # Determine the marker color based on the arrived date being within the earliest and latest date range\n",
    "        if earliest_date <= arrived_date <= latest_date:\n",
    "            marker_color = 'green'\n",
    "        else:\n",
    "            marker_color = 'red'\n",
    "\n",
    "        # Add an index label\n",
    "        folium.Marker([lat + offset * 0.000001, lon + offset * 0.000001], icon=folium.DivIcon(html=f'<div style=\"font-size: 13pt; border: 2px solid white; border-radius: 50%; padding: 2px; background-color: {marker_color}\">{i+1}</div>')).add_to(m)\n",
    "\n",
    "        # Add a red square marker for depots\n",
    "        if location_type == 1:\n",
    "            folium.Marker([lat + offset * 0.000001, lon + offset * 0.000001], icon=folium.Icon(color='red', icon='square', prefix='fa')).add_to(m)\n",
    "\n",
    "        if i < len(lat_seq) - 1:\n",
    "            next_lat, next_lon = lat_seq[i + 1], lon_seq[i + 1]\n",
    "            angle = np.rad2deg(np.arctan2(next_lat - lat, next_lon - lon))\n",
    "            arrow_lon = lon + 0.001 * np.cos(np.deg2rad(angle))\n",
    "\n",
    "            # Add a line segment\n",
    "            folium.PolyLine([[lat + offset * 0.000001, lon + offset * 0.000001], [next_lat, next_lon]], color=color, dash_array='5, 5').add_to(m)\n",
    "\n",
    "            # Add an arrow at the end of the line segment\n",
    "            folium.Marker([next_lat, next_lon], icon=folium.Icon(color=color, icon='arrow-up', prefix='fa')).add_to(m)\n",
    "\n",
    "        # Increase the offset for the next node with the same location\n",
    "        offset += 1\n",
    "\n",
    "    m.save(file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uncompleted_routes_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_planned_route(row_id):\n",
    "    lats = uncompleted_routes_df[\"current_lat\"].iloc[row_id]\n",
    "    lngs = uncompleted_routes_df[\"current_lng\"].iloc[row_id]\n",
    "    earliests = uncompleted_routes_df[\"stop_earliest\"].iloc[row_id]\n",
    "    latests = uncompleted_routes_df[\"stop_latest\"].iloc[row_id]\n",
    "    arriveds = uncompleted_routes_df[\"stop_arrived_at\"].iloc[row_id]\n",
    "    location_types = uncompleted_routes_df[\"location_type_id\"].iloc[row_id]\n",
    "    build_route(lats, lngs, 'blue', earliests, latests, arriveds, location_types, f'planned_route_{row_id}.html')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_actual_route(row_id):\n",
    "    mapping = {}\n",
    "    row = uncompleted_routes_df.iloc[row_id]\n",
    "    planned_route = row['planned_route_craft']\n",
    "    actual_route = row['actual_route_location']\n",
    "    longs = row['current_lng']\n",
    "    lats = row['current_lat']\n",
    "    earliest = row['stop_earliest']\n",
    "    latest = row['stop_latest']\n",
    "    arrived = row['stop_arrived_at']\n",
    "    location_type_id = row['location_type_id']\n",
    "\n",
    "    for i in range(len(planned_route)):\n",
    "        mapping[planned_route[i]] = (longs[i], lats[i], earliest[i], latest[i],arrived[i], location_type_id[i])\n",
    "\n",
    "    actual_longs = [mapping[location][0] for location in actual_route]\n",
    "    actual_lats = [mapping[location][1] for location in actual_route]\n",
    "    actual_earliest = [mapping[location][2] for location in actual_route]\n",
    "    actual_latest = [mapping[location][3] for location in actual_route]\n",
    "    actual_arrived = [mapping[location][4] for location in actual_route]\n",
    "    actual_location_type_id = [mapping[location][5] for location in actual_route]\n",
    "\n",
    "\n",
    "    build_route(actual_lats, actual_longs, 'red', actual_earliest, actual_latest, actual_arrived, actual_location_type_id, f'actual_route_{row_id}.html')\n",
    "\n",
    "id = 20\n",
    "draw_planned_route(id)\n",
    "draw_actual_route(id)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uncompleted_routes_df[\"current_lat\"].iloc[6], uncompleted_routes_df[\"current_lng\"].iloc[6]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# uncompleted_routes_df = uncompleted_routes_df.sample(n=len(completed_routes_df), random_state=42)\n",
    "len(completed_routes_df), len(uncompleted_routes_df)\n",
    "# completed_routes_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# completed_routes_list_unique = [array for i, array in enumerate(completed_routes_list) if array not in completed_routes_list[:i]]\n",
    "# uncompleted_routes_list_unique = [array for i, array in enumerate(uncompleted_routes_list) if array not in uncompleted_routes_list[:i]]\n",
    "# len(completed_routes_list_unique), len (uncompleted_routes_list_unique)\n",
    "# len(completed_routes_df), len(uncompleted_routes_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_routes = pd.DataFrame({\n",
    "    'routes': completed_routes_df['planned_route_craft'].tolist() + uncompleted_routes_df['planned_route_craft'].tolist(),\n",
    "    'driver_id': completed_routes_df['driver_id'].tolist() +  uncompleted_routes_df['driver_id'].tolist(),\n",
    "    'distance_route': completed_routes_df['distance_route'].tolist() + uncompleted_routes_df['distance_route'].tolist(),\n",
    "    'last_two_weeks_count': completed_routes_df['last_two_weeks_count'].tolist() + uncompleted_routes_df['last_two_weeks_count'].tolist(),\n",
    "    'current_lat': completed_routes_df['current_lat'].tolist() + uncompleted_routes_df['current_lat'].tolist(),\n",
    "    'current_lng': completed_routes_df['current_lng'].tolist() + uncompleted_routes_df['current_lng'].tolist(),\n",
    "    # 'day_of_week': completed_routes_df['day_of_week'].tolist() + uncompleted_routes_df['day_of_week'].tolist(),\n",
    "    'label': [0] * len(completed_routes_df)  + [1] * len(uncompleted_routes_df)\n",
    "})\n",
    "final_routes['len'] = final_routes['routes'].apply(lambda x: len(x))\n",
    "final_routes\n",
    "\n",
    "# [0] * len(artificial_planned_routes)\n",
    "# artificial_planned_routes['distance_route'].tolist()\n",
    "# artificial_planned_routes['driver_id'].tolist()\n",
    "# artificial_planned_routes['common_subsequence'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "max([len(i) for i in final_routes['routes']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create dictionary and encode to smaller unique numbers\n",
    "locations_dic = {}\n",
    "location_count = {}\n",
    "k = 1\n",
    "for row in final_routes['routes']:\n",
    "   for location in row:\n",
    "       if location not in locations_dic:\n",
    "           locations_dic[location] = k\n",
    "           k += 1\n",
    "\n",
    "print(len(locations_dic))\n",
    "\n",
    "for row in final_routes['routes']:\n",
    "   for location in row:\n",
    "       if location not in location_count:\n",
    "            location_count[location] = 1\n",
    "       else:\n",
    "            location_count[location] += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drivers_dic = {}\n",
    "k = 1\n",
    "for driver in final_routes['driver_id']:\n",
    "    if driver not in drivers_dic:\n",
    "        drivers_dic[driver] = k\n",
    "        k += 1\n",
    "print('Total number of drivers', len(drivers_dic))\n",
    "total_drivers = len(drivers_dic)\n",
    "encoding_drivers = []\n",
    "for driver in final_routes['driver_id']:\n",
    "    encoding_drivers.append(drivers_dic[driver])\n",
    "#\n",
    "final_routes['driver_id_sorted'] = encoding_drivers\n",
    "# final_routes = pd.concat([final_routes, pd.get_dummies(final_routes['driver_id_sorted'], prefix='encoding_drivers')], axis=1)\n",
    "final_routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_routes['experience_feature'] = final_routes.apply(lambda x: x['len'] * [x['last_two_weeks_count']], axis = 1)\n",
    "final_routes['len_feature'] = final_routes.apply(lambda x: x['len'] * [x['len']], axis = 1)\n",
    "final_routes['driver_id_feature'] = final_routes.apply(lambda x: x['len'] * [x['driver_id_sorted']], axis = 1)\n",
    "\n",
    "final_routes\n",
    "\n",
    "print('correlation', final_routes['len'].corr(final_routes['label']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "#\n",
    "# # One-hot encode categorical columns\n",
    "# encoded_routes = pd.get_dummies(final_routes.drop(['routes'], axis=1), drop_first=True)\n",
    "# # Train a Random Forest classifier\n",
    "# model = LogisticRegression()\n",
    "# model.fit(encoded_routes, final_routes['label'])\n",
    "# coefs = model.coef_\n",
    "#\n",
    "# # Calculate odds ratio\n",
    "# odds_ratios = np.exp(coefs)\n",
    "# odds_ratios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.stats import chi2_contingency\n",
    "#\n",
    "# # Convert categorical column to numerical representation\n",
    "# driver_ids = final_routes['driver_id_sorted'].astype('category')\n",
    "# driver_ids_encoded = driver_ids.cat.codes\n",
    "#\n",
    "# # Calculate contingency table\n",
    "# contingency_table = pd.crosstab(driver_ids_encoded, final_routes['label'])\n",
    "#\n",
    "# # Perform Chi-square test for independence\n",
    "# chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "#\n",
    "# # Print the correlation matrix\n",
    "# print(\"Correlation Matrix:\")\n",
    "# print(contingency_table)\n",
    "# print(f\"\\nChi-square statistic: {chi2_stat:.4f}\")\n",
    "# print(f\"p-value: {p_val:.4f}\")\n",
    "#\n",
    "# # Interpret the results\n",
    "# if p_val < 0.05:\n",
    "#     print(\"The driver_id and label columns are significantly associated.\")\n",
    "# else:\n",
    "#     print(\"The driver_id and label columns are not significantly associated.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "location_count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoding_routes = []\n",
    "for row in final_routes['routes']:\n",
    "    encoding_route = []\n",
    "    for location in row:\n",
    "        encoding_route.append(locations_dic[location])\n",
    "    encoding_routes.append(encoding_route)\n",
    "final_routes['routes'] = encoding_routes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def padding_(routes, route_len):\n",
    "    features = np.zeros((len(routes), route_len),dtype=np.float16)\n",
    "    for ii, route in enumerate(routes):\n",
    "        if len(route) != 0:\n",
    "            features[ii, -len(route):] = np.array(route)[:route_len]\n",
    "    return features\n",
    "\n",
    "X = final_routes.drop(columns = ['driver_id', 'label', 'last_two_weeks_count', 'len', 'driver_id_sorted'])\n",
    "# print(X)\n",
    "y = np.array(final_routes['label'])\n",
    "print(X)\n",
    "\n",
    "max_route_length = max(len(item) for item in X['routes'])\n",
    "# X = np.concatenate([padding_(X['routes'], max_route_length),padding_(X['distance_route'], max_route_length), X.to_numpy()[:,2:]], axis=1)\n",
    "X = np.concatenate([padding_(X['routes'], max_route_length),padding_(X['distance_route'], max_route_length), padding_(X['experience_feature'], max_route_length), padding_(X['len_feature'], max_route_length), padding_(X['driver_id_feature'], max_route_length)], axis=1)\n",
    "X = X.astype(np.float16)\n",
    "# X = X.astype(int) #for boolean values, to converst from string to int\n",
    "final_routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## List of features\n",
    "\n",
    "routes\n",
    "distance_route\n",
    "experience_feature\n",
    "len_feature\n",
    "driver_id_feature\n",
    "\n",
    "Total: 175\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "g = torch.Generator()\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Additional steps if using DataLoaders (to ensure reproducibility in data loading)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "def reset_random():\n",
    "    g.manual_seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch #pytorch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size, vocab_size_driv, hidden_dim,embedding_dim,embedding_dim_driv,output_dim,additional_feature_count,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_size_driv = vocab_size_driv\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_driv = nn.Embedding(vocab_size_driv, embedding_dim_driv)\n",
    "\n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim+embedding_dim_driv+1,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        # embedding_dim_driv+2\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        # self.fc = nn.Linear(self.hidden_dim + additional_feature_count, output_dim)\n",
    "        # self.fc = nn.Linear(self.hidden_dim + embedding_dim_driv, output_dim) #withfeatures\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        # self.fc = nn.Linear(36, output_dim)\n",
    "        # self.fc = nn.Linear(1316, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = len(x['routes'])\n",
    "        route_ids = x['routes'].int()\n",
    "\n",
    "        embeds = self.embedding(route_ids)  # shape: B x S x Feature   since batch = True\n",
    "\n",
    "        # get driver ids example\n",
    "        driver_ids = x['driver_id_feature'].int()\n",
    "        experience = x['experience_feature'].int()\n",
    "        # latitudes = x['current_lat'].int()\n",
    "        # longtitudes = x['current_lng'].int()\n",
    "        embedding_driv = self.embedding_driv(driver_ids)\n",
    "\n",
    "\n",
    "        all_embeds = torch.concatenate((embeds, embedding_driv, experience.view(batch_size, max_route_length, 1)), dim=2)\n",
    "\n",
    "        lstm_out, _ = self.lstm(all_embeds)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, max_route_length, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # print(out.shape)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        # print(sig_out.shape)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # print(sig_out.shape)\n",
    "        return sig_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    no_layers = 2\n",
    "    vocab_size = 2979 #extra 1 for padding\n",
    "    vocab_size_driv = 196\n",
    "    embedding_dim = 4 #was 64\n",
    "    embedding_dim_driv = 2 #was 64\n",
    "    output_dim = 1\n",
    "    hidden_dim = 32 #was 64\n",
    "\n",
    "\n",
    "    model = SentimentRNN(no_layers,vocab_size, vocab_size_driv, hidden_dim,embedding_dim, embedding_dim_driv, output_dim,len(drivers_dic),drop_prob=0.5)\n",
    "    model.train()\n",
    "    print(model)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "def get_precision(pred, label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    true_positive = torch.sum((pred == 1) & (label.squeeze() == 1)).item()\n",
    "    false_positive = torch.sum((pred == 1) & (label.squeeze() == 0)).item()\n",
    "\n",
    "    if true_positive + false_positive == 0:\n",
    "        return 0.0, true_positive, false_positive  # Handle the case where there are no predicted positives\n",
    "\n",
    "    precision_value = true_positive / (true_positive + false_positive)\n",
    "    return precision_value, true_positive, false_positive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(data, model, epochs = 20):\n",
    "    train_loader, valid_loader = data\n",
    "    lr=0.0005\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "    # train for some number of epochs\n",
    "    epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "    epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "    epoch_tr_precision,epoch_vl_precision = [],[]\n",
    "    run_name = 'first_run_4'\n",
    "    # wandb.init(project='Route_classification', name=f'{run_name}')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        train_acc = 0.0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            # print(labels)\n",
    "\n",
    "            model.zero_grad()\n",
    "            # print(inputs.shape, h[0].shape, h[1].shape)\n",
    "            output = model(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            # print(output, labels)\n",
    "            # print(output.shape, labels.shape)\n",
    "            loss = criterion(output.view(-1), labels.float())\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            # calculating accuracy\n",
    "            accuracy = acc(output,labels)\n",
    "            precision = get_precision(output,labels)\n",
    "            train_acc += accuracy\n",
    "            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        val_losses = []\n",
    "        val_acc = 0.0\n",
    "        val_precision = 0.0\n",
    "        val_tp = 0\n",
    "        val_fp = 0\n",
    "        model.eval()\n",
    "        for inputs, labels in valid_loader:\n",
    "                # inputs, labels = inputs.to(device), labels.to(device)\n",
    "                output = model(inputs)\n",
    "                val_loss = criterion(output.view(-1), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                accuracy = acc(output,labels)\n",
    "                precision = get_precision(output,labels)\n",
    "                val_acc += accuracy\n",
    "                val_precision += precision[0]\n",
    "                val_tp += precision[1]\n",
    "                val_fp += precision[2]\n",
    "\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_val_loss = np.mean(val_losses)\n",
    "        epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "        epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "        print(val_tp, val_fp)\n",
    "        if val_tp + val_fp == 0:\n",
    "            print('sum zero')\n",
    "            val_prec = 0\n",
    "        else:\n",
    "            val_prec = val_tp / (val_tp + val_fp)\n",
    "        epoch_tr_loss.append(epoch_train_loss)\n",
    "        epoch_vl_loss.append(epoch_val_loss)\n",
    "        epoch_tr_acc.append(epoch_train_acc)\n",
    "        epoch_vl_acc.append(epoch_val_acc)\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "        print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "        print(f'val_precision : {val_prec * 100}')\n",
    "        # wandb.log({\n",
    "        #     'epoch_train_loss': epoch_train_loss,\n",
    "        #     'epoch_val_loss': epoch_val_loss,\n",
    "        #     'epoch_train_acc': epoch_train_acc*100,\n",
    "        #     'epoch_val_acc': epoch_val_acc*100,\n",
    "        #     'epoch_val_precision': val_prec*100\n",
    "        # })\n",
    "        if epoch_val_loss <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), f'{run_name}.pt')\n",
    "            # torch.save(model.state_dict(), os.path.join(wandb.run.dir, f'{run_name}.pt'))\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "            valid_loss_min = epoch_val_loss\n",
    "        print(25*'==')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_stats(model, data):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_pred_exact = []\n",
    "    y = []\n",
    "    for inputs, labels in data:\n",
    "        y.extend(labels)\n",
    "        results = model(inputs).detach()\n",
    "        y_pred.extend(np.round(results))\n",
    "        y_pred_exact.extend(results)\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    quadratic_loss = np.mean((1-np.array(y))*(1-np.array(y_pred_exact))**2 + np.array(y)*(np.array(y_pred_exact)**2))\n",
    "    brier_score = np.mean((np.array(y_pred_exact) - np.array(y)) ** 2)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_exact)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # precision, recall, thresholds = precision_recall_curve(y, y_pred_exact)\n",
    "    average_precision = average_precision_score(y, y_pred_exact)\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\" : roc_auc,\n",
    "        \"average_precision\": average_precision,\n",
    "        \"conf_matrix\": conf_mat,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"quadratic_loss\": quadratic_loss,\n",
    "        \"brier_score\": brier_score\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_value)\n",
    "\n",
    "stats = []\n",
    "\n",
    "FEATURE_COLUMNS = ['routes', 'driver_id_feature','experience_feature']\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for col in FEATURE_COLUMNS:\n",
    "            feature = self.df[col].iloc[idx]\n",
    "            if isinstance(feature, list):\n",
    "                feature = padding_([feature], max_route_length)[0]\n",
    "            item[col] = feature\n",
    "        item['label'] = self.df['label'].iloc[idx]\n",
    "        return item, item['label']\n",
    "\n",
    "def get_data_loaders(train_df, test_df):\n",
    "    # create Tensor datasets\n",
    "    train_data = DataFrameDataset(train_df)\n",
    "    valid_data = DataFrameDataset(test_df)\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 64\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, generator=g, worker_init_fn=seed_worker)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, generator=g, worker_init_fn=seed_worker)\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "shuffled_df = final_routes.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "for train_index, test_index in kf.split(shuffled_df, shuffled_df['label']):\n",
    "    train_df = shuffled_df.iloc[train_index]\n",
    "    test_df = shuffled_df.iloc[test_index]\n",
    "\n",
    "    train_loader, valid_loader = get_data_loaders(train_df, test_df)\n",
    "    print(f'len(train_df) = {len(train_df)}, len(test_df) = {len(test_df)}')\n",
    "    model = get_model()\n",
    "    model.train()\n",
    "\n",
    "    train_model((train_loader, valid_loader), model, epochs=50)\n",
    "    stats.append(get_stats(model, valid_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mean_stat(stats, stat_name):\n",
    "    if stat_name in [\"fpr\", \"tpr\"]:\n",
    "        # Calculate mean fpr and tpr\n",
    "        values = [item[stat_name] for item in stats]\n",
    "        min_len = min(len(value) for value in values)\n",
    "        interpolated_values = []\n",
    "        for value in values:\n",
    "            interpolated = np.interp(np.linspace(0, 1, min_len), np.linspace(0, 1, len(value)), value)\n",
    "            interpolated_values.append(interpolated)\n",
    "        mean_values = np.array(interpolated_values).mean(axis=0)\n",
    "        return mean_values\n",
    "    else:\n",
    "        # Calculate mean for other stats\n",
    "        return np.array([item[stat_name] for item in stats]).mean()\n",
    "\n",
    "print('acc:', get_mean_stat(stats, 'acc'))\n",
    "print('precision:', get_mean_stat(stats, 'precision'))\n",
    "print('recall:', get_mean_stat(stats, 'recall'))\n",
    "print('f1:', get_mean_stat(stats, 'f1'))\n",
    "print('roc_auc:', get_mean_stat(stats, 'roc_auc'))\n",
    "print('average_precision:', get_mean_stat(stats, 'average_precision'))\n",
    "print('quadratic_loss:', get_mean_stat(stats, 'quadratic_loss'))\n",
    "print('brier_score:', get_mean_stat(stats, 'brier_score'))\n",
    "mean_fpr = get_mean_stat(stats, 'fpr')\n",
    "mean_tpr = get_mean_stat(stats, 'tpr')\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# reset_random()\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state=seed_value)\n",
    "# train_loader, valid_loader = get_data_loaders(X_train, y_train, X_val, y_val)\n",
    "#\n",
    "# model = get_model()\n",
    "# # summary(model, input_size=(1,9))\n",
    "# model.train()\n",
    "#\n",
    "# train_model((train_loader, valid_loader), model, epochs=50)\n",
    "#\n",
    "# get_stats(model, valid_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_stats(model, train_loader)\n",
    "# def get_mean_stat(stats, stat_name) -> float:\n",
    "#     return np.array([item[stat_name] for item in stats]).mean()\n",
    "#\n",
    "# print('acc:', get_mean_stat(stats, 'acc'))\n",
    "# print('precision:', get_mean_stat(stats, 'precision'))\n",
    "# print('recall:', get_mean_stat(stats, 'recall'))\n",
    "# print('f1:', get_mean_stat(stats, 'f1'))\n",
    "# print('roc_auc:', get_mean_stat(stats, 'roc_auc'))\n",
    "# print('average_precision:', get_mean_stat(stats, 'average_precision'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.fc.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}