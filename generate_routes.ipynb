{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generate routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_completed_at        0\n",
      "stop_arrived_at          0\n",
      "stop_latest              0\n",
      "stop_earliest            0\n",
      "stop_dispatched_at       0\n",
      "location_id              0\n",
      "location_type_id         0\n",
      "driver_workday_id        0\n",
      "organization_id          0\n",
      "address_id               0\n",
      "location_is_depot        0\n",
      "driver_id                0\n",
      "contact_id               0\n",
      "current_lat              0\n",
      "current_lng              0\n",
      "prev_planned_lat      7450\n",
      "prev_planned_lng      7450\n",
      "prev_actual_lat       7450\n",
      "prev_actual_lng       7450\n",
      "dtype: int64\n",
      "number of groups 3110\n"
     ]
    },
    {
     "data": {
      "text/plain": "      driver_workday_id                                          driver_id  \\\n0                297900  [1255, 1255, 1255, 1255, 1255, 1255, 1255, 125...   \n1                297906         [1195, 1195, 1195, 1195, 1195, 1195, 1195]   \n2                297908  [1176, 1176, 1176, 1176, 1176, 1176, 1176, 117...   \n3                297911  [1197, 1197, 1197, 1197, 1197, 1197, 1197, 119...   \n4                297919  [1218, 1218, 1218, 1218, 1218, 1218, 1218, 121...   \n...                 ...                                                ...   \n7445             378095  [1318, 1318, 1318, 1318, 1318, 1318, 1318, 131...   \n7446             378097                           [1320, 1320, 1320, 1320]   \n7447             378098  [1504, 1504, 1504, 1504, 1504, 1504, 1504, 150...   \n7448             378393   [1576, 1576, 1576, 1576, 1576, 1576, 1576, 1576]   \n7449             378751  [1226, 1226, 1226, 1226, 1226, 1226, 1226, 122...   \n\n                                       location_type_id  \\\n0                        [1, 2, 2, 2, 2, 2, 2, 2, 2, 2]   \n1                                 [1, 2, 2, 2, 2, 2, 2]   \n2     [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n3                        [1, 2, 2, 2, 2, 2, 2, 2, 2, 2]   \n4                     [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]   \n...                                                 ...   \n7445               [1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2]   \n7446                                       [1, 2, 1, 2]   \n7447                        [1, 2, 2, 2, 2, 2, 2, 2, 2]   \n7448                           [1, 2, 2, 2, 2, 2, 2, 2]   \n7449                     [1, 1, 2, 2, 2, 2, 2, 2, 2, 2]   \n\n                                 planned_route_location  \\\n0     [122267, 118973, 118854, 118820, 118623, 11909...   \n1     [122267, 120295, 120297, 118930, 120298, 13328...   \n2     [122267, 120515, 119217, 119215, 119170, 13445...   \n3     [122267, 145327, 133259, 133259, 133259, 13325...   \n4     [122267, 119426, 118790, 119069, 118805, 11885...   \n...                                                 ...   \n7445  [153184, 157688, 159697, 166402, 119926, 15318...   \n7446                   [153184, 126176, 119202, 119283]   \n7447  [153184, 158743, 118952, 118885, 118639, 11947...   \n7448  [153184, 157689, 118782, 156136, 118827, 15770...   \n7449  [153184, 153184, 119765, 119762, 119741, 11868...   \n\n                                     stop_dispatched_at  \\\n0     [2024-01-01 19:41:35.136881+00, 2024-01-01 19:...   \n1     [2024-01-01 19:42:05.147145+00, 2024-01-01 19:...   \n2     [2024-01-01 21:25:47.230131+00, 2024-01-01 21:...   \n3     [2024-01-01 19:42:59.704147+00, 2024-01-01 19:...   \n4     [2024-01-01 19:46:51.16173+00, 2024-01-01 19:4...   \n...                                                 ...   \n7445  [2024-04-27 03:09:22.847874+00, 2024-04-27 03:...   \n7446  [2024-04-27 02:47:23.809951+00, 2024-04-27 02:...   \n7447  [2024-04-27 04:12:56.973126+00, 2024-04-27 04:...   \n7448  [2024-04-27 05:24:23.574508+00, 2024-04-27 05:...   \n7449  [2024-04-28 19:29:11.353248+00, 2024-04-28 19:...   \n\n                                        stop_arrived_at  \\\n0     [2024-01-01 22:14:16.454324+00, 2024-01-01 23:...   \n1     [2024-01-01 20:50:17.003744+00, 2024-01-02 02:...   \n2     [2024-01-01 23:06:27.627368+00, 2024-01-02 01:...   \n3     [2024-01-02 00:09:15.241119+00, 2024-01-02 04:...   \n4     [2024-01-01 23:05:52.039818+00, 2024-01-02 01:...   \n...                                                 ...   \n7445  [2024-04-27 03:57:22.361031+00, 2024-04-27 05:...   \n7446  [2024-04-27 05:01:26.330777+00, 2024-04-27 05:...   \n7447  [2024-04-27 11:31:06.496202+00, 2024-04-27 11:...   \n7448  [2024-04-27 05:57:37.333989+00, 2024-04-27 06:...   \n7449  [2024-04-28 19:29:55.504638+00, 2024-04-28 21:...   \n\n                                          stop_earliest  \\\n0     [2024-01-01 19:00:00+00, 2024-01-01 23:00:00+0...   \n1     [2024-01-01 19:00:00+00, 2024-01-01 21:00:00+0...   \n2     [2024-01-01 19:00:00+00, 2024-01-01 21:00:00+0...   \n3     [2024-01-02 00:00:00+00, 2024-01-02 04:30:00+0...   \n4     [2024-01-01 19:00:00+00, 2024-01-01 21:00:00+0...   \n...                                                 ...   \n7445  [2024-04-26 23:00:00+00, 2024-04-27 03:00:00+0...   \n7446  [2024-04-26 23:00:00+00, 2024-04-27 04:00:00+0...   \n7447  [2024-04-26 23:00:00+00, 2024-04-27 05:00:00+0...   \n7448  [2024-04-26 23:00:00+00, 2024-04-27 06:00:00+0...   \n7449  [2024-04-28 18:00:00+00, 2024-04-28 18:00:00+0...   \n\n                                            stop_latest  \\\n0     [2024-01-02 01:00:00+00, 2024-01-02 03:00:00+0...   \n1     [2024-01-02 01:00:00+00, 2024-01-02 04:00:00+0...   \n2     [2024-01-02 01:00:00+00, 2024-01-02 04:00:00+0...   \n3     [2024-01-02 22:00:00+00, 2024-01-02 08:00:00+0...   \n4     [2024-01-02 01:00:00+00, 2024-01-02 04:00:00+0...   \n...                                                 ...   \n7445  [2024-04-27 21:00:00+00, 2024-04-27 08:00:00+0...   \n7446  [2024-04-27 21:00:00+00, 2024-04-27 10:00:00+0...   \n7447  [2024-04-27 21:00:00+00, 2024-04-27 08:00:00+0...   \n7448  [2024-04-27 21:00:00+00, 2024-04-27 11:00:00+0...   \n7449  [2024-04-29 00:00:00+00, 2024-04-29 00:00:00+0...   \n\n                                      stop_completed_at  \\\n0     [2024-01-01 22:14:54.872731+00, 2024-01-01 23:...   \n1     [2024-01-01 20:50:29.721122+00, 2024-01-02 02:...   \n2     [2024-01-01 23:06:36.064172+00, 2024-01-02 01:...   \n3     [2024-01-02 00:09:21.131516+00, 2024-01-02 04:...   \n4     [2024-01-01 23:05:58.283704+00, 2024-01-02 01:...   \n...                                                 ...   \n7445  [2024-04-27 03:57:31.310281+00, 2024-04-27 05:...   \n7446  [2024-04-27 05:01:34.392346+00, 2024-04-27 05:...   \n7447  [2024-04-27 11:31:06.496209+00, 2024-04-27 11:...   \n7448  [2024-04-27 05:57:49.42596+00, 2024-04-27 06:5...   \n7449  [2024-04-28 21:00:05.016089+00, 2024-04-28 21:...   \n\n                                  planned_route_cluster  \\\n0     [766, 597, 919, 560, 560, 404, 811, 404, 404, ...   \n1                   [766, 723, 723, 263, 116, 116, 416]   \n2     [766, 625, 785, 376, 506, 626, 854, 854, 854, ...   \n3          [766, 188, 39, 39, 39, 39, 39, 39, 957, 188]   \n4     [766, 687, 160, 829, 386, 408, 408, 843, 348, ...   \n...                                                 ...   \n7445  [4, 919, 848, 848, 848, 4, 4, 595, 427, 4, 811...   \n7446                                  [4, 31, 854, 595]   \n7447         [4, 69, 679, 784, 707, 747, 815, 395, 263]   \n7448               [4, 848, 53, 848, 53, 666, 848, 386]   \n7449     [4, 4, 511, 741, 276, 741, 427, 973, 863, 911]   \n\n                                    planned_route_craft  \\\n0     [886, 1060, 1171, 1160, 1098, 1057, 1078, 1065...   \n1              [886, 1018, 1019, 1116, 1020, 989, 1051]   \n2     [886, 721, 790, 772, 737, 695, 970, 970, 960, ...   \n3               [886, 5, 20, 20, 20, 20, 20, 20, 10, 3]   \n4     [886, 726, 624, 808, 1226, 891, 880, 678, 651,...   \n...                                                 ...   \n7445  [878, 1162, 1216, 1190, 1175, 878, 878, 842, 7...   \n7446                               [878, 162, 980, 825]   \n7447  [878, 1348, 1316, 1248, 1201, 1366, 1300, 1465...   \n7448    [878, 1189, 1187, 1174, 1164, 1128, 1165, 1225]   \n7449  [878, 878, 577, 529, 543, 542, 709, 688, 1363,...   \n\n                                            day_of_week  \\\n0     [Monday, Monday, Monday, Monday, Monday, Monda...   \n1     [Monday, Monday, Monday, Monday, Monday, Monda...   \n2     [Monday, Monday, Monday, Monday, Monday, Monda...   \n3     [Monday, Monday, Monday, Monday, Monday, Monda...   \n4     [Monday, Monday, Monday, Monday, Monday, Monda...   \n...                                                 ...   \n7445  [Saturday, Saturday, Saturday, Saturday, Satur...   \n7446           [Saturday, Saturday, Saturday, Saturday]   \n7447  [Saturday, Saturday, Saturday, Saturday, Satur...   \n7448  [Saturday, Saturday, Saturday, Saturday, Satur...   \n7449  [Sunday, Sunday, Sunday, Sunday, Sunday, Sunda...   \n\n                                                   date  \n0     [2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...  \n1     [2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...  \n2     [2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...  \n3     [2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...  \n4     [2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...  \n...                                                 ...  \n7445  [2024-04-27, 2024-04-27, 2024-04-27, 2024-04-2...  \n7446   [2024-04-27, 2024-04-27, 2024-04-27, 2024-04-27]  \n7447  [2024-04-27, 2024-04-27, 2024-04-27, 2024-04-2...  \n7448  [2024-04-27, 2024-04-27, 2024-04-27, 2024-04-2...  \n7449  [2024-04-28, 2024-04-28, 2024-04-28, 2024-04-2...  \n\n[7450 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>driver_workday_id</th>\n      <th>driver_id</th>\n      <th>location_type_id</th>\n      <th>planned_route_location</th>\n      <th>stop_dispatched_at</th>\n      <th>stop_arrived_at</th>\n      <th>stop_earliest</th>\n      <th>stop_latest</th>\n      <th>stop_completed_at</th>\n      <th>planned_route_cluster</th>\n      <th>planned_route_craft</th>\n      <th>day_of_week</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>297900</td>\n      <td>[1255, 1255, 1255, 1255, 1255, 1255, 1255, 125...</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n      <td>[122267, 118973, 118854, 118820, 118623, 11909...</td>\n      <td>[2024-01-01 19:41:35.136881+00, 2024-01-01 19:...</td>\n      <td>[2024-01-01 22:14:16.454324+00, 2024-01-01 23:...</td>\n      <td>[2024-01-01 19:00:00+00, 2024-01-01 23:00:00+0...</td>\n      <td>[2024-01-02 01:00:00+00, 2024-01-02 03:00:00+0...</td>\n      <td>[2024-01-01 22:14:54.872731+00, 2024-01-01 23:...</td>\n      <td>[766, 597, 919, 560, 560, 404, 811, 404, 404, ...</td>\n      <td>[886, 1060, 1171, 1160, 1098, 1057, 1078, 1065...</td>\n      <td>[Monday, Monday, Monday, Monday, Monday, Monda...</td>\n      <td>[2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>297906</td>\n      <td>[1195, 1195, 1195, 1195, 1195, 1195, 1195]</td>\n      <td>[1, 2, 2, 2, 2, 2, 2]</td>\n      <td>[122267, 120295, 120297, 118930, 120298, 13328...</td>\n      <td>[2024-01-01 19:42:05.147145+00, 2024-01-01 19:...</td>\n      <td>[2024-01-01 20:50:17.003744+00, 2024-01-02 02:...</td>\n      <td>[2024-01-01 19:00:00+00, 2024-01-01 21:00:00+0...</td>\n      <td>[2024-01-02 01:00:00+00, 2024-01-02 04:00:00+0...</td>\n      <td>[2024-01-01 20:50:29.721122+00, 2024-01-02 02:...</td>\n      <td>[766, 723, 723, 263, 116, 116, 416]</td>\n      <td>[886, 1018, 1019, 1116, 1020, 989, 1051]</td>\n      <td>[Monday, Monday, Monday, Monday, Monday, Monda...</td>\n      <td>[2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>297908</td>\n      <td>[1176, 1176, 1176, 1176, 1176, 1176, 1176, 117...</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n      <td>[122267, 120515, 119217, 119215, 119170, 13445...</td>\n      <td>[2024-01-01 21:25:47.230131+00, 2024-01-01 21:...</td>\n      <td>[2024-01-01 23:06:27.627368+00, 2024-01-02 01:...</td>\n      <td>[2024-01-01 19:00:00+00, 2024-01-01 21:00:00+0...</td>\n      <td>[2024-01-02 01:00:00+00, 2024-01-02 04:00:00+0...</td>\n      <td>[2024-01-01 23:06:36.064172+00, 2024-01-02 01:...</td>\n      <td>[766, 625, 785, 376, 506, 626, 854, 854, 854, ...</td>\n      <td>[886, 721, 790, 772, 737, 695, 970, 970, 960, ...</td>\n      <td>[Monday, Monday, Monday, Monday, Monday, Monda...</td>\n      <td>[2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>297911</td>\n      <td>[1197, 1197, 1197, 1197, 1197, 1197, 1197, 119...</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n      <td>[122267, 145327, 133259, 133259, 133259, 13325...</td>\n      <td>[2024-01-01 19:42:59.704147+00, 2024-01-01 19:...</td>\n      <td>[2024-01-02 00:09:15.241119+00, 2024-01-02 04:...</td>\n      <td>[2024-01-02 00:00:00+00, 2024-01-02 04:30:00+0...</td>\n      <td>[2024-01-02 22:00:00+00, 2024-01-02 08:00:00+0...</td>\n      <td>[2024-01-02 00:09:21.131516+00, 2024-01-02 04:...</td>\n      <td>[766, 188, 39, 39, 39, 39, 39, 39, 957, 188]</td>\n      <td>[886, 5, 20, 20, 20, 20, 20, 20, 10, 3]</td>\n      <td>[Monday, Monday, Monday, Monday, Monday, Monda...</td>\n      <td>[2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>297919</td>\n      <td>[1218, 1218, 1218, 1218, 1218, 1218, 1218, 121...</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n      <td>[122267, 119426, 118790, 119069, 118805, 11885...</td>\n      <td>[2024-01-01 19:46:51.16173+00, 2024-01-01 19:4...</td>\n      <td>[2024-01-01 23:05:52.039818+00, 2024-01-02 01:...</td>\n      <td>[2024-01-01 19:00:00+00, 2024-01-01 21:00:00+0...</td>\n      <td>[2024-01-02 01:00:00+00, 2024-01-02 04:00:00+0...</td>\n      <td>[2024-01-01 23:05:58.283704+00, 2024-01-02 01:...</td>\n      <td>[766, 687, 160, 829, 386, 408, 408, 843, 348, ...</td>\n      <td>[886, 726, 624, 808, 1226, 891, 880, 678, 651,...</td>\n      <td>[Monday, Monday, Monday, Monday, Monday, Monda...</td>\n      <td>[2024-01-01, 2024-01-01, 2024-01-01, 2024-01-0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7445</th>\n      <td>378095</td>\n      <td>[1318, 1318, 1318, 1318, 1318, 1318, 1318, 131...</td>\n      <td>[1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2]</td>\n      <td>[153184, 157688, 159697, 166402, 119926, 15318...</td>\n      <td>[2024-04-27 03:09:22.847874+00, 2024-04-27 03:...</td>\n      <td>[2024-04-27 03:57:22.361031+00, 2024-04-27 05:...</td>\n      <td>[2024-04-26 23:00:00+00, 2024-04-27 03:00:00+0...</td>\n      <td>[2024-04-27 21:00:00+00, 2024-04-27 08:00:00+0...</td>\n      <td>[2024-04-27 03:57:31.310281+00, 2024-04-27 05:...</td>\n      <td>[4, 919, 848, 848, 848, 4, 4, 595, 427, 4, 811...</td>\n      <td>[878, 1162, 1216, 1190, 1175, 878, 878, 842, 7...</td>\n      <td>[Saturday, Saturday, Saturday, Saturday, Satur...</td>\n      <td>[2024-04-27, 2024-04-27, 2024-04-27, 2024-04-2...</td>\n    </tr>\n    <tr>\n      <th>7446</th>\n      <td>378097</td>\n      <td>[1320, 1320, 1320, 1320]</td>\n      <td>[1, 2, 1, 2]</td>\n      <td>[153184, 126176, 119202, 119283]</td>\n      <td>[2024-04-27 02:47:23.809951+00, 2024-04-27 02:...</td>\n      <td>[2024-04-27 05:01:26.330777+00, 2024-04-27 05:...</td>\n      <td>[2024-04-26 23:00:00+00, 2024-04-27 04:00:00+0...</td>\n      <td>[2024-04-27 21:00:00+00, 2024-04-27 10:00:00+0...</td>\n      <td>[2024-04-27 05:01:34.392346+00, 2024-04-27 05:...</td>\n      <td>[4, 31, 854, 595]</td>\n      <td>[878, 162, 980, 825]</td>\n      <td>[Saturday, Saturday, Saturday, Saturday]</td>\n      <td>[2024-04-27, 2024-04-27, 2024-04-27, 2024-04-27]</td>\n    </tr>\n    <tr>\n      <th>7447</th>\n      <td>378098</td>\n      <td>[1504, 1504, 1504, 1504, 1504, 1504, 1504, 150...</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n      <td>[153184, 158743, 118952, 118885, 118639, 11947...</td>\n      <td>[2024-04-27 04:12:56.973126+00, 2024-04-27 04:...</td>\n      <td>[2024-04-27 11:31:06.496202+00, 2024-04-27 11:...</td>\n      <td>[2024-04-26 23:00:00+00, 2024-04-27 05:00:00+0...</td>\n      <td>[2024-04-27 21:00:00+00, 2024-04-27 08:00:00+0...</td>\n      <td>[2024-04-27 11:31:06.496209+00, 2024-04-27 11:...</td>\n      <td>[4, 69, 679, 784, 707, 747, 815, 395, 263]</td>\n      <td>[878, 1348, 1316, 1248, 1201, 1366, 1300, 1465...</td>\n      <td>[Saturday, Saturday, Saturday, Saturday, Satur...</td>\n      <td>[2024-04-27, 2024-04-27, 2024-04-27, 2024-04-2...</td>\n    </tr>\n    <tr>\n      <th>7448</th>\n      <td>378393</td>\n      <td>[1576, 1576, 1576, 1576, 1576, 1576, 1576, 1576]</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2]</td>\n      <td>[153184, 157689, 118782, 156136, 118827, 15770...</td>\n      <td>[2024-04-27 05:24:23.574508+00, 2024-04-27 05:...</td>\n      <td>[2024-04-27 05:57:37.333989+00, 2024-04-27 06:...</td>\n      <td>[2024-04-26 23:00:00+00, 2024-04-27 06:00:00+0...</td>\n      <td>[2024-04-27 21:00:00+00, 2024-04-27 11:00:00+0...</td>\n      <td>[2024-04-27 05:57:49.42596+00, 2024-04-27 06:5...</td>\n      <td>[4, 848, 53, 848, 53, 666, 848, 386]</td>\n      <td>[878, 1189, 1187, 1174, 1164, 1128, 1165, 1225]</td>\n      <td>[Saturday, Saturday, Saturday, Saturday, Satur...</td>\n      <td>[2024-04-27, 2024-04-27, 2024-04-27, 2024-04-2...</td>\n    </tr>\n    <tr>\n      <th>7449</th>\n      <td>378751</td>\n      <td>[1226, 1226, 1226, 1226, 1226, 1226, 1226, 122...</td>\n      <td>[1, 1, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n      <td>[153184, 153184, 119765, 119762, 119741, 11868...</td>\n      <td>[2024-04-28 19:29:11.353248+00, 2024-04-28 19:...</td>\n      <td>[2024-04-28 19:29:55.504638+00, 2024-04-28 21:...</td>\n      <td>[2024-04-28 18:00:00+00, 2024-04-28 18:00:00+0...</td>\n      <td>[2024-04-29 00:00:00+00, 2024-04-29 00:00:00+0...</td>\n      <td>[2024-04-28 21:00:05.016089+00, 2024-04-28 21:...</td>\n      <td>[4, 4, 511, 741, 276, 741, 427, 973, 863, 911]</td>\n      <td>[878, 878, 577, 529, 543, 542, 709, 688, 1363,...</td>\n      <td>[Sunday, Sunday, Sunday, Sunday, Sunday, Sunda...</td>\n      <td>[2024-04-28, 2024-04-28, 2024-04-28, 2024-04-2...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7450 rows Ã— 13 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stops = pd.read_csv('uni_molde_v2.csv', sep=',')\n",
    "\n",
    "\n",
    "data_stops.loc[data_stops['stop_completed_at'].isna(), 'stop_completed_at'] = \"-1\"\n",
    "data_stops.loc[data_stops['stop_arrived_at'].isna(), 'stop_arrived_at'] = \"-1\"\n",
    "\n",
    "print(data_stops.isnull().sum())\n",
    "\n",
    "\n",
    "sorted_data_stops = data_stops.sort_values(by='stop_dispatched_at', ascending=True)\n",
    "sorted_data_stops = sorted_data_stops.reset_index(drop=True)\n",
    "sorted_data_stops['day_of_week'] = pd.to_datetime(sorted_data_stops['stop_dispatched_at']).dt.day_name()\n",
    "sorted_data_stops['date'] = pd.to_datetime(sorted_data_stops['stop_dispatched_at']).dt.date\n",
    "\n",
    "#clustering\n",
    "locations_df = sorted_data_stops[['current_lat', 'current_lng']]\n",
    "kmeans = KMeans(n_clusters=1000, random_state=42)\n",
    "kmeans.fit(locations_df)\n",
    "sorted_data_stops['cluster'] = kmeans.labels_ + 1\n",
    "\n",
    "sorted_data_stops['location_id_craft'] = sorted_data_stops.groupby(['current_lat', 'current_lng']).ngroup()+1\n",
    "\n",
    "print('number of groups', sorted_data_stops['location_id_craft'].nunique())\n",
    "\n",
    "# data_stops_day= sorted_data_stops[sorted_data_stops['day_of_week'] == \"Wednesday\"]\n",
    "grouped_df = sorted_data_stops.groupby('driver_workday_id')[['driver_id', 'location_type_id', 'address_id', 'stop_dispatched_at', 'stop_arrived_at', 'stop_earliest', 'stop_latest', 'stop_completed_at', 'cluster', 'location_id_craft','day_of_week', 'date']].apply(lambda x: pd.Series({\n",
    "    'driver_id': x['driver_id'].tolist(),\n",
    "    'location_type_id': x['location_type_id'].tolist(),\n",
    "    'planned_route_location': x['address_id'].tolist(),\n",
    "    'stop_dispatched_at': x['stop_dispatched_at'].tolist(),\n",
    "    'stop_arrived_at': x['stop_arrived_at'].tolist(),\n",
    "    'stop_earliest': x['stop_earliest'].tolist(),\n",
    "    'stop_latest': x['stop_latest'].tolist(),\n",
    "    'stop_completed_at': x['stop_completed_at'].tolist(),\n",
    "    'planned_route_cluster': x['cluster'].tolist(),\n",
    "    'planned_route_craft': x['location_id_craft'].tolist(),\n",
    "    'day_of_week': x['day_of_week'].tolist(),\n",
    "    'date': x['date'].tolist()\n",
    "})).reset_index()\n",
    "grouped_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m distances\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Create a new column 'distance_route' in 'final_routes'\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m grouped_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistance_route\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mgrouped_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcalculate_distance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m  10360\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[1;32m  10362\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[1;32m  10363\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m  10364\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  10372\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[1;32m  10373\u001B[0m )\n\u001B[0;32m> 10374\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/apply.py:916\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw(engine\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine, engine_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine_kwargs)\n\u001B[0;32m--> 916\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1061\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1062\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1063\u001B[0m         results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1064\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1065\u001B[0m         results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_series_numba()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1078\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1079\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[1;32m   1080\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[0;32m-> 1081\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1082\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[1;32m   1083\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[1;32m   1084\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[1;32m   1085\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[3], line 10\u001B[0m, in \u001B[0;36mcalculate_distance\u001B[0;34m(row)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplanned_route_craft\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m      8\u001B[0m     coords_1 \u001B[38;5;241m=\u001B[39m (sorted_data_stops\u001B[38;5;241m.\u001B[39mloc[sorted_data_stops[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation_id_craft\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplanned_route_craft\u001B[39m\u001B[38;5;124m'\u001B[39m][i], [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent_lat\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m      9\u001B[0m                  sorted_data_stops\u001B[38;5;241m.\u001B[39mloc[sorted_data_stops[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation_id_craft\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplanned_route_craft\u001B[39m\u001B[38;5;124m'\u001B[39m][i], [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent_lng\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 10\u001B[0m     coords_2 \u001B[38;5;241m=\u001B[39m (\u001B[43msorted_data_stops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43msorted_data_stops\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlocation_id_craft\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mplanned_route_craft\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcurrent_lat\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m     11\u001B[0m                  sorted_data_stops\u001B[38;5;241m.\u001B[39mloc[sorted_data_stops[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation_id_craft\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplanned_route_craft\u001B[39m\u001B[38;5;124m'\u001B[39m][i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m], [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent_lng\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     12\u001B[0m     distances\u001B[38;5;241m.\u001B[39mappend(geodesic(coords_1, coords_2)\u001B[38;5;241m.\u001B[39mmiles)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m distances\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/indexing.py:1184\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_scalar_access(key):\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39m_get_value(\u001B[38;5;241m*\u001B[39mkey, takeable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_takeable)\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;66;03m# we by definition only have the 0th axis\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m     axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/indexing.py:1377\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_tuple\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_multi_take_opportunity(tup):\n\u001B[1;32m   1375\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_multi_take(tup)\n\u001B[0;32m-> 1377\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_tuple_same_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtup\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/indexing.py:1020\u001B[0m, in \u001B[0;36m_LocationIndexer._getitem_tuple_same_dim\u001B[0;34m(self, tup)\u001B[0m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_null_slice(key):\n\u001B[1;32m   1018\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m-> 1020\u001B[0m retval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mretval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001B[39;00m\n\u001B[1;32m   1022\u001B[0m \u001B[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001B[39;00m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m retval\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mndim\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/indexing.py:1420\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1417\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mndim\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1418\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot index with multidimensional key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1420\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_iterable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1422\u001B[0m \u001B[38;5;66;03m# nested tuple slicing\u001B[39;00m\n\u001B[1;32m   1423\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_nested_tuple(key, labels):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/indexing.py:1361\u001B[0m, in \u001B[0;36m_LocIndexer._getitem_iterable\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1359\u001B[0m \u001B[38;5;66;03m# A collection of keys\u001B[39;00m\n\u001B[1;32m   1360\u001B[0m keyarr, indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_listlike_indexer(key, axis)\n\u001B[0;32m-> 1361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reindex_with_indexers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1362\u001B[0m \u001B[43m    \u001B[49m\u001B[43m{\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_dups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m   1363\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/generic.py:5706\u001B[0m, in \u001B[0;36mNDFrame._reindex_with_indexers\u001B[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001B[0m\n\u001B[1;32m   5703\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m using_copy_on_write() \u001B[38;5;129;01mand\u001B[39;00m new_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr:\n\u001B[1;32m   5704\u001B[0m     new_data \u001B[38;5;241m=\u001B[39m new_data\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m-> 5706\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_constructor_from_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxes\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__finalize__\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5707\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\n\u001B[1;32m   5708\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/generic.py:6262\u001B[0m, in \u001B[0;36mNDFrame.__finalize__\u001B[0;34m(self, other, method, **kwargs)\u001B[0m\n\u001B[1;32m   6255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m other\u001B[38;5;241m.\u001B[39mattrs:\n\u001B[1;32m   6256\u001B[0m     \u001B[38;5;66;03m# We want attrs propagation to have minimal performance\u001B[39;00m\n\u001B[1;32m   6257\u001B[0m     \u001B[38;5;66;03m# impact if attrs are not used; i.e. attrs is an empty dict.\u001B[39;00m\n\u001B[1;32m   6258\u001B[0m     \u001B[38;5;66;03m# One could make the deepcopy unconditionally, but a deepcopy\u001B[39;00m\n\u001B[1;32m   6259\u001B[0m     \u001B[38;5;66;03m# of an empty dict is 50x more expensive than the empty check.\u001B[39;00m\n\u001B[1;32m   6260\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrs \u001B[38;5;241m=\u001B[39m deepcopy(other\u001B[38;5;241m.\u001B[39mattrs)\n\u001B[0;32m-> 6262\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mallows_duplicate_labels \u001B[38;5;241m=\u001B[39m \u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflags\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mallows_duplicate_labels\u001B[49m\n\u001B[1;32m   6263\u001B[0m \u001B[38;5;66;03m# For subclasses using _metadata.\u001B[39;00m\n\u001B[1;32m   6264\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(other\u001B[38;5;241m.\u001B[39m_metadata):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RealDataClassificator/lib/python3.10/site-packages/pandas/core/flags.py:55\u001B[0m, in \u001B[0;36mFlags.allows_duplicate_labels\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_allows_duplicate_labels \u001B[38;5;241m=\u001B[39m allows_duplicate_labels\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj \u001B[38;5;241m=\u001B[39m weakref\u001B[38;5;241m.\u001B[39mref(obj)\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mallows_duplicate_labels\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m     57\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m    Whether this object allows duplicate labels.\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03m    a        [0, 1]\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_allows_duplicate_labels\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from geopy.distance import geodesic\n",
    "import pandas as pd\n",
    "\n",
    "# Function to calculate distance between two coordinates\n",
    "def calculate_distance(row):\n",
    "    distances = []\n",
    "    for i in range(len(row['planned_route_craft'])-1):\n",
    "        coords_1 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i], ['current_lng']].values[0][0])\n",
    "        coords_2 = (sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i+1], ['current_lat']].values[0][0],\n",
    "                     sorted_data_stops.loc[sorted_data_stops['location_id_craft'] == row['planned_route_craft'][i+1], ['current_lng']].values[0][0])\n",
    "        distances.append(geodesic(coords_1, coords_2).miles)\n",
    "    return distances\n",
    "\n",
    "# Create a new column 'distance_route' in 'final_routes'\n",
    "grouped_df['distance_route'] = grouped_df.apply(calculate_distance, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "# sse = []\n",
    "#\n",
    "# for i in range(1, 21):\n",
    "#     # create a KMeans model with the current number of clusters\n",
    "#     kmeans = KMeans(n_clusters=i)\n",
    "#     # fit the model to the locations dataframe\n",
    "#     kmeans.fit(sorted_data_stops[['current_lat', 'current_lng']])\n",
    "#     # append the sum of squared errors to the list\n",
    "#     sse.append(kmeans.inertia_)\n",
    "#\n",
    "# # create a plot with the number of clusters on the x-axis and SSE on the y-axis\n",
    "# plt.plot(range(1, 21), sse)\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Sum of Squared Errors (SSE)')\n",
    "# plt.title('Elbow Plot')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes = grouped_df[grouped_df.apply(lambda row: max(row['stop_dispatched_at']) < min(row['stop_completed_at']), axis=1)]\n",
    "routes = routes.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(routes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index_routes_with_na = []\n",
    "for i in range(len(routes)):\n",
    "    row = routes.iloc[i]\n",
    "    if \"-1\" in row['stop_arrived_at']:\n",
    "        index_routes_with_na.append(i)\n",
    "print(\"The number of routes where one value is NA(arrived time)\", len(index_routes_with_na))\n",
    "print(index_routes_with_na)\n",
    "routes = routes.drop(index_routes_with_na)\n",
    "routes.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#check if it is the same driver in the route\n",
    "for i in routes['driver_id']:\n",
    "    if not all(x == i[0] for x in i):\n",
    "            print('Not the same driver in the route')\n",
    "routes['driver_id'] = routes['driver_id'].apply(lambda x : x[0])\n",
    "\n",
    "routes['day_of_week'] = routes['day_of_week'].apply(lambda x : x[0])\n",
    "routes['date'] = routes['date'].apply(lambda x : x[0])\n",
    "\n",
    "routes['date'] = pd.to_datetime(routes['date'])\n",
    "routes['last_two_weeks_count'] = routes.apply(lambda row:\n",
    "                                      routes[(routes['driver_id'] == row['driver_id']) &\n",
    "                                         (row['date'] - routes['date']).dt.days.between(-14, 0)].shape[0],\n",
    "                                      axis=1)\n",
    "routes = routes[(routes['date'] < '2024-01-01') | (routes['date'] > '2024-01-14')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_actual_route(df, column):\n",
    "    res_col = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        sorted_dates = sorted(row['stop_arrived_at'])\n",
    "        mapping = {}\n",
    "        for i, date in enumerate(sorted_dates):\n",
    "            mapping[date] = row[column][i]\n",
    "        res_val = [mapping[row['stop_arrived_at'][i]] for i in range(len(row['stop_arrived_at']))]\n",
    "        res_col.append(res_val)\n",
    "    return res_col\n",
    "\n",
    "routes['actual_route_location'] = create_actual_route(routes, 'planned_route_craft')\n",
    "routes.to_csv('routes.csv', sep=';')\n",
    "routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "routes[['planned_route_craft', 'actual_route_location']]\n",
    "\n",
    "import itertools\n",
    "artificial_completed_routes = pd.DataFrame(columns=['routes'])\n",
    "\n",
    "def common_subsequence(planned, actual):\n",
    "    return [x[0] for x in itertools.takewhile(lambda x: x[0] == x[1], zip(planned, actual))]\n",
    "\n",
    "artificial_completed_routes['routes'] = routes.apply(lambda row: common_subsequence(row['planned_route_craft'], row['actual_route_location']), axis=1)\n",
    "\n",
    "filtered_results = artificial_completed_routes[artificial_completed_routes['routes'].apply(lambda x: len(x) > 1 and len(x) != len(routes.loc[artificial_completed_routes.index, 'planned_route_craft']))]\n",
    "\n",
    "filtered_results['driver_id'] = routes.loc[filtered_results.index, 'driver_id']\n",
    "filtered_results['last_two_weeks_count'] = routes.loc[filtered_results.index, 'last_two_weeks_count']\n",
    "\n",
    "filtered_results['distance_route'] = filtered_results.apply(lambda row: routes.loc[row.name, 'distance_route'][:len(row['routes'])-1], axis=1)\n",
    "\n",
    "artificial_planned_routes = pd.DataFrame({'common_subsequence': filtered_results['routes'], 'driver_id': filtered_results['driver_id'], 'distance_route': filtered_results['distance_route'], 'last_two_weeks_count': filtered_results['last_two_weeks_count']})\n",
    "\n",
    "artificial_planned_routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "planned_routes = routes[['planned_route_craft', 'driver_id', 'day_of_week', 'distance_route', 'last_two_weeks_count']]\n",
    "actual_routes = routes['actual_route_location']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "planned_routes_list = planned_routes['planned_route_craft'].tolist()\n",
    "actual_routes_list = actual_routes.tolist()\n",
    "\n",
    "# filtered_rows = []\n",
    "#\n",
    "# for row in actual_routes_list:\n",
    "#     if row not in planned_routes_list:\n",
    "#         filtered_rows.append(row)\n",
    "\n",
    "#duplicated removed\n",
    "# planned_routes_list = [array for i, array in enumerate(planned_routes_list) if array not in planned_routes_list[:i]]\n",
    "# actual_routes_list = [array for i, array in enumerate(filtered_rows) if array not in filtered_rows[:i]]\n",
    "len(planned_routes_list),len(actual_routes_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Edit distance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Edit Distance\n",
    "\n",
    "def minDistance(word1, word2) -> int:\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    # dp[i][j] := min # Of operations to convert word1[0..i) to word2[0..j)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "      dp[i][0] = i\n",
    "\n",
    "    for j in range(1, n + 1):\n",
    "      dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "      for j in range(1, n + 1):\n",
    "        if word1[i - 1] == word2[j - 1]:\n",
    "          dp[i][j] = dp[i - 1][j - 1]\n",
    "        else:\n",
    "          dp[i][j] = min(dp[i - 1][j - 1], dp[i - 1][j], dp[i][j - 1]) + 1\n",
    "\n",
    "    return dp[m][n]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each driver-day, we consider only the planned route. For each planned route we calculate how much it deviates from the actual route (e.g., use some form of edit distance - see https://link.springer.com/article/10.1007/s10732-006-9001-3?), which is then normalized (divide by max edit distance, or number of visits or something?) to the interval [0, 1]. Then, a planned route is good if this distance (between planned and actual) is less than a certain threshold and otherwise it is bad. This becomes the label (bad/good)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "completed_routes_df = pd.DataFrame(columns=['planned_route_craft', 'driver_id', 'day_of_week', 'distance_route', 'last_two_weeks_count'])\n",
    "uncompleted_routes_df = pd.DataFrame(columns=['planned_route_craft', 'driver_id', 'day_of_week', 'distance_route', 'last_two_weeks_count'])\n",
    "#11700795\n",
    "for i in range(len(planned_routes_list)):\n",
    "    if minDistance(planned_routes_list[i],actual_routes_list[i]) < 6:\n",
    "        completed_routes_df = pd.concat([completed_routes_df, pd.DataFrame([planned_routes.iloc[i]]).reset_index(drop=True)], ignore_index=True)\n",
    "    else:\n",
    "        uncompleted_routes_df = pd.concat([uncompleted_routes_df, pd.DataFrame([planned_routes.iloc[i]]).reset_index(drop=True)], ignore_index=True)\n",
    "\n",
    "\n",
    "completed_routes_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(completed_routes_df), len(uncompleted_routes_df)\n",
    "# completed_routes_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# completed_routes_list_unique = [array for i, array in enumerate(completed_routes_list) if array not in completed_routes_list[:i]]\n",
    "# uncompleted_routes_list_unique = [array for i, array in enumerate(uncompleted_routes_list) if array not in uncompleted_routes_list[:i]]\n",
    "# len(completed_routes_list_unique), len (uncompleted_routes_list_unique)\n",
    "# len(completed_routes_df), len(uncompleted_routes_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_routes = pd.DataFrame({\n",
    "    'routes': completed_routes_df['planned_route_craft'].tolist() + uncompleted_routes_df['planned_route_craft'].tolist(),\n",
    "    'driver_id': completed_routes_df['driver_id'].tolist() +  uncompleted_routes_df['driver_id'].tolist(),\n",
    "    'distance_route': completed_routes_df['distance_route'].tolist() + uncompleted_routes_df['distance_route'].tolist(),\n",
    "    'last_two_weeks_count': completed_routes_df['last_two_weeks_count'].tolist() + uncompleted_routes_df['last_two_weeks_count'].tolist(),\n",
    "    # 'day_of_week': completed_routes_df['day_of_week'].tolist() + uncompleted_routes_df['day_of_week'].tolist(),\n",
    "    'label': [0] * len(completed_routes_df)  + [1] * len(uncompleted_routes_df)\n",
    "})\n",
    "final_routes['len'] = final_routes['routes'].apply(lambda x: len(x))\n",
    "final_routes\n",
    "\n",
    "# [0] * len(artificial_planned_routes)\n",
    "# artificial_planned_routes['distance_route'].tolist()\n",
    "# artificial_planned_routes['driver_id'].tolist()\n",
    "# artificial_planned_routes['common_subsequence'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "max([len(i) for i in final_routes['routes']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create dictionary and encode to smaller unique numbers\n",
    "locations_dic = {}\n",
    "location_count = {}\n",
    "k = 0\n",
    "for row in final_routes['routes']:\n",
    "   for location in row:\n",
    "       if location not in locations_dic:\n",
    "           locations_dic[location] = k\n",
    "           k += 1\n",
    "\n",
    "print(len(locations_dic))\n",
    "\n",
    "for row in final_routes['routes']:\n",
    "   for location in row:\n",
    "       if location not in location_count:\n",
    "            location_count[location] = 1\n",
    "       else:\n",
    "            location_count[location] += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drivers_dic = {}\n",
    "k = 1\n",
    "for driver in final_routes['driver_id']:\n",
    "    if driver not in drivers_dic:\n",
    "        drivers_dic[driver] = k\n",
    "        k += 1\n",
    "print('Total number of drivers', len(drivers_dic))\n",
    "total_drivers = len(drivers_dic)\n",
    "encoding_drivers = []\n",
    "for driver in final_routes['driver_id']:\n",
    "    encoding_drivers.append(drivers_dic[driver])\n",
    "#\n",
    "final_routes['driver_id_sorted'] = encoding_drivers\n",
    "# final_routes = pd.concat([final_routes, pd.get_dummies(final_routes['driver_id_sorted'], prefix='encoding_drivers')], axis=1)\n",
    "final_routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_routes['experience_feature'] = final_routes.apply(lambda x: x['len'] * [x['last_two_weeks_count']], axis = 1)\n",
    "final_routes['len_feature'] = final_routes.apply(lambda x: x['len'] * [x['len']], axis = 1)\n",
    "final_routes['driver_id_feature'] = final_routes.apply(lambda x: x['len'] * [x['driver_id_sorted']], axis = 1)\n",
    "\n",
    "final_routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "#\n",
    "# # One-hot encode categorical columns\n",
    "# encoded_routes = pd.get_dummies(final_routes.drop(['routes'], axis=1), drop_first=True)\n",
    "# # Train a Random Forest classifier\n",
    "# model = LogisticRegression()\n",
    "# model.fit(encoded_routes, final_routes['label'])\n",
    "# coefs = model.coef_\n",
    "#\n",
    "# # Calculate odds ratio\n",
    "# odds_ratios = np.exp(coefs)\n",
    "# odds_ratios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.stats import chi2_contingency\n",
    "#\n",
    "# # Convert categorical column to numerical representation\n",
    "# driver_ids = final_routes['driver_id_sorted'].astype('category')\n",
    "# driver_ids_encoded = driver_ids.cat.codes\n",
    "#\n",
    "# # Calculate contingency table\n",
    "# contingency_table = pd.crosstab(driver_ids_encoded, final_routes['label'])\n",
    "#\n",
    "# # Perform Chi-square test for independence\n",
    "# chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "#\n",
    "# # Print the correlation matrix\n",
    "# print(\"Correlation Matrix:\")\n",
    "# print(contingency_table)\n",
    "# print(f\"\\nChi-square statistic: {chi2_stat:.4f}\")\n",
    "# print(f\"p-value: {p_val:.4f}\")\n",
    "#\n",
    "# # Interpret the results\n",
    "# if p_val < 0.05:\n",
    "#     print(\"The driver_id and label columns are significantly associated.\")\n",
    "# else:\n",
    "#     print(\"The driver_id and label columns are not significantly associated.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "location_count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoding_routes = []\n",
    "for row in final_routes['routes']:\n",
    "    encoding_route = []\n",
    "    for location in row:\n",
    "        encoding_route.append(locations_dic[location])\n",
    "    encoding_routes.append(encoding_route)\n",
    "final_routes['routes'] = encoding_routes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_routes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def padding_(routes, route_len):\n",
    "    features = np.zeros((len(routes), route_len),dtype=np.float16)\n",
    "    for ii, route in enumerate(routes):\n",
    "        if len(route) != 0:\n",
    "            features[ii, -len(route):] = np.array(route)[:route_len]\n",
    "    return features\n",
    "\n",
    "X = final_routes.drop(columns = ['driver_id', 'label', 'last_two_weeks_count', 'len', 'driver_id_sorted'])\n",
    "# print(X)\n",
    "y = np.array(final_routes['label'])\n",
    "print(X)\n",
    "\n",
    "max_route_length = max(len(item) for item in X['routes'])\n",
    "# X = np.concatenate([padding_(X['routes'], max_route_length),padding_(X['distance_route'], max_route_length), X.to_numpy()[:,2:]], axis=1)\n",
    "X = np.concatenate([padding_(X['routes'], max_route_length),padding_(X['distance_route'], max_route_length), padding_(X['experience_feature'], max_route_length), padding_(X['len_feature'], max_route_length), padding_(X['driver_id_feature'], max_route_length)], axis=1)\n",
    "X = X.astype(np.float16)\n",
    "# X = X.astype(int) #for boolean values, to converst from string to int\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## List of features\n",
    "\n",
    "routes\n",
    "distance_route\n",
    "experience_feature\n",
    "len_feature\n",
    "driver_id_feature\n",
    "\n",
    "Total: 175\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "seed_value = 42\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "g = torch.Generator()\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Additional steps if using DataLoaders (to ensure reproducibility in data loading)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "def reset_random():\n",
    "    g.manual_seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def get_data_loaders(x_train, y_train, x_test, y_test):\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "    valid_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 64\n",
    "\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, generator=g, worker_init_fn=seed_worker)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, generator=g, worker_init_fn=seed_worker)\n",
    "    return train_loader, valid_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch #pytorch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size, vocab_size_driv, hidden_dim,embedding_dim,embedding_dim_driv,output_dim,additional_feature_count,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab_size_driv = vocab_size_driv\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_driv = nn.Embedding(vocab_size_driv, embedding_dim_driv)\n",
    "\n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        # embedding_dim_driv+2\n",
    "        # self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "        #                    num_layers=no_layers, batch_first=True)\n",
    "\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        # linear and sigmoid layer\n",
    "        # self.fc = nn.Linear(self.hidden_dim + additional_feature_count, output_dim)\n",
    "        # self.fc = nn.Linear(self.hidden_dim + embedding_dim_driv, output_dim) #withfeatures\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        # self.fc = nn.Linear(1316, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        # print('b', batch_size)\n",
    "        # embeddings and lstm_out\n",
    "        route_ids = x[:, :max_route_length].int()\n",
    "        embeds = self.embedding(route_ids)  # shape: B x S x Feature   since batch = True\n",
    "\n",
    "        drivers_ids = x[:, 4 * max_route_length: ].int()\n",
    "        embeds_driv = self.embedding_driv(drivers_ids)  # shape: B x S x Feature   since batch = True\n",
    "\n",
    "        # embeds_driv_extend = torch.tensor(np.tile(embeds_driv.detach().numpy(), (1, max_route_length))).reshape((batch_size, max_route_length, 2))   ###TODO HARD ENCODING ATTENTION\n",
    "\n",
    "        # static_feature_extend = torch.tensor(np.tile(x[:, 2*max_route_length: 2*max_route_length+2], (1, max_route_length))).reshape((batch_size, max_route_length, 2))\n",
    "        # x[:, max_route_length: 2*max_route_length].view(batch_size, max_route_length, 1),\n",
    "        # x[:, 2*max_route_length: 3*max_route_length].view(batch_size, max_route_length, 1),\n",
    "        # x[:, 3*max_route_length: 4*max_route_length].view(batch_size, max_route_length, 1)\n",
    "        lstm_input = torch.cat((embeds, x[:, max_route_length: 2*max_route_length].view(batch_size, max_route_length, 1).float(), x[:, 2*max_route_length: 3*max_route_length].view(batch_size, max_route_length, 1).float(),x[:, 3*max_route_length: 4*max_route_length].view(batch_size, max_route_length, 1).float(), embeds_driv), dim=2)\n",
    "        # embeds_driv_extend, static_feature_extend\n",
    "\n",
    "        # lstm_out, _ = self.lstm(embeds)\n",
    "        lstm_out, _ = self.lstm(embeds) #with features\n",
    "        # print('l', lstm_out.shape)\n",
    "\n",
    "        # test\n",
    "        # lstm_out = lstm_out[:, -1, :]\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, max_route_length, self.hidden_dim)\n",
    "        # print('l2', lstm_out.shape)\n",
    "\n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        # print('o', out.shape)\n",
    "        # print(out.shape, x[:, max_route_length:].shape)\n",
    "        # torch.Size([64, 35, 32]) torch.Size([64, 196])\n",
    "        out_last = out[:, -1, :].view(-1, self.hidden_dim)\n",
    "        # out_last = out.view(batch_size, -1)\n",
    "        # all_features = torch.cat((out_last, x[:, max_route_length:]), dim=1)\n",
    "        # all_features = torch.cat((out_last, embeds_driv), dim=1)\n",
    "        # out = self.fc(all_features) #with features\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        # print('s', sig_out.shape)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        # print('s2', sig_out.shape)\n",
    "        # print('sig_out',sig_out)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        # print('s3', sig_out.shape)\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    no_layers = 2\n",
    "    vocab_size = 2979 #extra 1 for padding\n",
    "    vocab_size_driv = 196\n",
    "    embedding_dim = 4 #was 64\n",
    "    embedding_dim_driv = 2 #was 64\n",
    "    output_dim = 1\n",
    "    hidden_dim = 32 #was 64\n",
    "\n",
    "\n",
    "    model = SentimentRNN(no_layers,vocab_size, vocab_size_driv, hidden_dim,embedding_dim, embedding_dim_driv, output_dim,len(drivers_dic),drop_prob=0.5)\n",
    "    model.train()\n",
    "    print(model)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "def get_precision(pred, label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    true_positive = torch.sum((pred == 1) & (label.squeeze() == 1)).item()\n",
    "    false_positive = torch.sum((pred == 1) & (label.squeeze() == 0)).item()\n",
    "\n",
    "    if true_positive + false_positive == 0:\n",
    "        return 0.0, true_positive, false_positive  # Handle the case where there are no predicted positives\n",
    "\n",
    "    precision_value = true_positive / (true_positive + false_positive)\n",
    "    return precision_value, true_positive, false_positive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_model(data, model, epochs = 20):\n",
    "    train_loader, valid_loader = data\n",
    "    lr=0.0005\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    clip = 5\n",
    "    valid_loss_min = np.Inf\n",
    "    # train for some number of epochs\n",
    "    epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "    epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "    epoch_tr_precision,epoch_vl_precision = [],[]\n",
    "    run_name = 'first_run_4'\n",
    "    # wandb.init(project='Route_classification', name=f'{run_name}')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        train_acc = 0.0\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            # print(inputs.shape, h[0].shape, h[1].shape)\n",
    "            output = model(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            # print(output, labels)\n",
    "            loss = criterion(output.view(-1), labels.float())\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            # calculating accuracy\n",
    "            accuracy = acc(output,labels)\n",
    "            precision = get_precision(output,labels)\n",
    "            train_acc += accuracy\n",
    "            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        val_losses = []\n",
    "        val_acc = 0.0\n",
    "        val_precision = 0.0\n",
    "        val_tp = 0\n",
    "        val_fp = 0\n",
    "        model.eval()\n",
    "        for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                output = model(inputs)\n",
    "                val_loss = criterion(output.view(-1), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                accuracy = acc(output,labels)\n",
    "                precision = get_precision(output,labels)\n",
    "                val_acc += accuracy\n",
    "                val_precision += precision[0]\n",
    "                val_tp += precision[1]\n",
    "                val_fp += precision[2]\n",
    "\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_val_loss = np.mean(val_losses)\n",
    "        epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "        epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "        print(val_tp, val_fp)\n",
    "        if val_tp + val_fp == 0:\n",
    "            print('sum zero')\n",
    "            val_prec = 0\n",
    "        else:\n",
    "            val_prec = val_tp / (val_tp + val_fp)\n",
    "        epoch_tr_loss.append(epoch_train_loss)\n",
    "        epoch_vl_loss.append(epoch_val_loss)\n",
    "        epoch_tr_acc.append(epoch_train_acc)\n",
    "        epoch_vl_acc.append(epoch_val_acc)\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "        print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "        print(f'val_precision : {val_prec * 100}')\n",
    "        # wandb.log({\n",
    "        #     'epoch_train_loss': epoch_train_loss,\n",
    "        #     'epoch_val_loss': epoch_val_loss,\n",
    "        #     'epoch_train_acc': epoch_train_acc*100,\n",
    "        #     'epoch_val_acc': epoch_val_acc*100,\n",
    "        #     'epoch_val_precision': val_prec*100\n",
    "        # })\n",
    "        if epoch_val_loss <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), f'{run_name}.pt')\n",
    "            # torch.save(model.state_dict(), os.path.join(wandb.run.dir, f'{run_name}.pt'))\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "            valid_loss_min = epoch_val_loss\n",
    "        print(25*'==')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_stats(model, data):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for inputs, labels in data:\n",
    "        x.extend(inputs)\n",
    "        y.extend(labels)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    model.eval()\n",
    "    y_pred = np.round(model(torch.tensor(x)).detach())\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "\n",
    "    y_pred_exact = model(torch.tensor(x)).detach()\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred_exact)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # precision, recall, thresholds = precision_recall_curve(y, y_pred_exact)\n",
    "    average_precision = average_precision_score(y, y_pred_exact)\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\" : roc_auc,\n",
    "        \"average_precision\": average_precision,\n",
    "        \"conf_matrix\": conf_mat,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed_value)\n",
    "\n",
    "# indices = np.arange(X.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# X = X[indices]\n",
    "# y = y[indices]\n",
    "\n",
    "stats = []\n",
    "# split()  method generate indices to split data into training and test set\n",
    "print(len(X))\n",
    "len_art = len(artificial_planned_routes)\n",
    "\n",
    "#regular k_folding\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_val = X[train_index], X[test_index]\n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "\n",
    "    train_loader, valid_loader = get_data_loaders(X_train_shuffled, y_train_shuffled, X_val, y_val)\n",
    "    print('len of validation, X(val)', len(X_val), len(X_train))\n",
    "    model = get_model()\n",
    "    # summary(model, input_size=(1,9))\n",
    "    model.train()\n",
    "\n",
    "    train_model((train_loader, valid_loader), model, epochs=20)\n",
    "    stats.append(get_stats(model, valid_loader))\n",
    "\n",
    "#with reserving some routes\n",
    "#Split data into training and testing sets, excluding artificial data from testing\n",
    "\n",
    "# for train_idx, test_idx in kf.split(X[:-len_art], y[:-len_art]):\n",
    "# for train_idx, test_idx in kf.split(X, y):\n",
    "#     X_train, X_val = X[train_idx], X[test_idx]\n",
    "#     y_train, y_val = y[train_idx], y[test_idx]\n",
    "#\n",
    "#     # Append artificial data to training set\n",
    "#     # X_train = np.concatenate((X_train, X[-len_art:]), axis=0)\n",
    "#     # y_train = np.concatenate((y_train, y[-len_art:]), axis=0)\n",
    "#\n",
    "#     indices = np.arange(X_train.shape[0])\n",
    "#     np.random.shuffle(indices)\n",
    "#     X_train_shuffled = X_train[indices]\n",
    "#     y_train_shuffled = y_train[indices]\n",
    "#\n",
    "#     print(len(X_train_shuffled), len(y_train_shuffled), len(X_val))\n",
    "#\n",
    "#     len_subset = int(0.95 * len(X_train_shuffled))\n",
    "#     x_subset = X_train_shuffled[:-len_subset]\n",
    "#     y_subset = y_train_shuffled[:-len_subset]\n",
    "#     train_loader, valid_loader = get_data_loaders(x_subset,y_subset, X_val, y_val)\n",
    "#\n",
    "#     print('len of validation, len of training, X(val)', len_subset, len(x_subset), len(y_subset))\n",
    "#\n",
    "#     model = get_model()\n",
    "#     model.train()\n",
    "#\n",
    "#     train_model((train_loader, valid_loader), model, epochs=30)\n",
    "#     stats.append(get_stats(model, valid_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mean_stat(stats, stat_name):\n",
    "    if stat_name in [\"fpr\", \"tpr\"]:\n",
    "        # Calculate mean fpr and tpr\n",
    "        values = [item[stat_name] for item in stats]\n",
    "        min_len = min(len(value) for value in values)\n",
    "        interpolated_values = []\n",
    "        for value in values:\n",
    "            interpolated = np.interp(np.linspace(0, 1, min_len), np.linspace(0, 1, len(value)), value)\n",
    "            interpolated_values.append(interpolated)\n",
    "        mean_values = np.array(interpolated_values).mean(axis=0)\n",
    "        return mean_values\n",
    "    else:\n",
    "        # Calculate mean for other stats\n",
    "        return np.array([item[stat_name] for item in stats]).mean()\n",
    "\n",
    "print('acc:', get_mean_stat(stats, 'acc'))\n",
    "print('precision:', get_mean_stat(stats, 'precision'))\n",
    "print('recall:', get_mean_stat(stats, 'recall'))\n",
    "print('f1:', get_mean_stat(stats, 'f1'))\n",
    "print('roc_auc:', get_mean_stat(stats, 'roc_auc'))\n",
    "print('average_precision:', get_mean_stat(stats, 'average_precision'))\n",
    "mean_fpr = get_mean_stat(stats, 'fpr')\n",
    "mean_tpr = get_mean_stat(stats, 'tpr')\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reset_random()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state=seed_value)\n",
    "train_loader, valid_loader = get_data_loaders(X_train, y_train, X_val, y_val)\n",
    "\n",
    "model = get_model()\n",
    "# summary(model, input_size=(1,9))\n",
    "model.train()\n",
    "\n",
    "train_model((train_loader, valid_loader), model, epochs=50)\n",
    "\n",
    "get_stats(model, valid_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_stats(model, train_loader)\n",
    "# def get_mean_stat(stats, stat_name) -> float:\n",
    "#     return np.array([item[stat_name] for item in stats]).mean()\n",
    "#\n",
    "# print('acc:', get_mean_stat(stats, 'acc'))\n",
    "# print('precision:', get_mean_stat(stats, 'precision'))\n",
    "# print('recall:', get_mean_stat(stats, 'recall'))\n",
    "# print('f1:', get_mean_stat(stats, 'f1'))\n",
    "# print('roc_auc:', get_mean_stat(stats, 'roc_auc'))\n",
    "# print('average_precision:', get_mean_stat(stats, 'average_precision'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kf.split(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}